{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# исследование произвеодительности хранилищ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uuid import UUID, uuid4\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from clickhouse_driver import Client, connect\n",
    "import psycopg2\n",
    "import pymongo\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from functools import wraps\n",
    "from pydantic import BaseConfig\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import psycopg2\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=13\n",
    "TOTAL_RECORDS= 100_000\n",
    "CHUNK_SIZE=1_000\n",
    "N_USERS=1_000\n",
    "ACTIONS = [\n",
    "    'start_watch',\n",
    "    'stop_watch',\n",
    "    'continue_watch',\n",
    "    'like',\n",
    "    'dislike',\n",
    "    'comment',\n",
    "    'add_to_favorite',\n",
    "    'delete_from_favorite',  \n",
    "    'user_login',\n",
    "    'user_logout'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Input:\n",
    "    id: int\n",
    "    user_id: int\n",
    "    timestamp: int\n",
    "    action: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DatabaseBenchmarkRunnerBase(ABC):\n",
    " \n",
    "    @abstractmethod\n",
    "    def run_write(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run_read(self, chunk_size):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "class VerticaBenchmarkRunner(DatabaseBenchmarkRunnerBase): \n",
    "    \n",
    "    def __init__(self, db_uri)  :\n",
    "        pass\n",
    "    \n",
    "    def run_write(self, data):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "    def run_read(self, chunk_size):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "class PostgresBenchmarkRunner(DatabaseBenchmarkRunnerBase): \n",
    "    \n",
    "    def __init__(self, db_uri)  :\n",
    "        pass\n",
    "    \n",
    "    def run_write(self, data):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "    def run_read(self, chunk_size):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "class MongoBenchmarkRunner(DatabaseBenchmarkRunnerBase): \n",
    "    \n",
    "    def __init__(self, db_uri)  :\n",
    "        pass\n",
    "    \n",
    "    def run_write(self, data):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "    def run_read(self, chunk_size):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "class ElasticBenchmarkRunner(DatabaseBenchmarkRunnerBase): \n",
    "    \n",
    "    def __init__(self, db_uri)  :\n",
    "        pass\n",
    "    \n",
    "    def run_write(self, data):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "    def run_read(self, chunk_size):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "class RedisBenchmarkRunner(DatabaseBenchmarkRunnerBase): \n",
    "    \n",
    "    def __init__(self, db_uri)  :\n",
    "        pass\n",
    "    \n",
    "    def run_write(self, data):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "    def run_read(self, chunk_size):\n",
    "        duration_seconds = np.random.uniform(.001, 1)\n",
    "        return duration_seconds\n",
    "    \n",
    "    \n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, seed, n_users, total_records, actions):\n",
    "        self.seed = seed \n",
    "        self.n_users = n_users\n",
    "        self.total_records = total_records        \n",
    "        self.actions=actions    \n",
    "        self.user_ids = np.int32(np.rint(np.random.uniform(0, 1e6, size=self.n_users)))      \n",
    "        \n",
    "    def generate_chunk_data(self, chunk_size):\n",
    "        np.random.seed(self.seed)        \n",
    "        n_chunks = np.int32(np.ceil(self.total_records/chunk_size))\n",
    "        for _ in tqdm_notebook(np.arange(n_chunks)): \n",
    "            np.random.seed(self.seed+_+1)\n",
    "            ids = np.int32(np.rint(np.random.uniform(0, 1e6, size=chunk_size)))\n",
    "            np.random.seed(self.seed+_+2)\n",
    "            user_ids = np.random.choice(self.user_ids, size=chunk_size)\n",
    "            np.random.seed(self.seed+_+3)\n",
    "            timestamps = np.int32(np.rint(np.random.uniform(0, 1e6, size=chunk_size)))\n",
    "            np.random.seed(self.seed+_+4)\n",
    "            actions    = np.random.choice(self.actions, size=chunk_size)\n",
    "            data       = np.column_stack([ids, user_ids, timestamps, actions]).tolist()\n",
    "            df = pd.DataFrame(data)\n",
    "            del data\n",
    "            df[[0, 1, 2]]  = df[[0, 1, 2]].astype('int')\n",
    "            data = list(df.itertuples(index=False, name=None))\n",
    "            del df\n",
    "            data = list(map(lambda row: Input(id=row[0], user_id=row[1], timestamp=row[2], action=row[3]), data))\n",
    "            yield data\n",
    "            del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vertica_python\n",
    "\n",
    "clickhouse_client = Client(host='localhost', port=9000)\n",
    "clickhouse_client.execute('DROP TABLE IF EXISTS test;') \n",
    "clickhouse_client.execute('CREATE TABLE IF NOT EXISTS test(id Int32, user_id Int32, timestamp Float32, payload String) ENGINE = Memory')\n",
    "\n",
    "conn_info = {'host': '127.0.0.1',\n",
    "             'port': 5433,\n",
    "             'user': 'test',             \n",
    "}\n",
    "with vertica_python.connect(**conn_info) as conn:\n",
    "    cur = conn.cursor()    \n",
    "    cur.execute('DROP TABLE IF EXISTS test;') \n",
    "    cur.execute('CREATE TABLE IF NOT EXISTS test(id int, user_id int, timestamp float, payload varchar)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clickhouse(clickhouse_client, records_to_insert):\n",
    "    records_list_template = ','.join(['%s'] * len(records_to_insert))\n",
    "    insert_query = 'insert into test (id, user_id, timestamp, payload) values {}'.format(records_to_insert)   \n",
    "    start = time.time()\n",
    "    clickhouse_client.execute('INSERT INTO test (id, user_id, timestamp, payload) VALUES', records_to_insert)\n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def read_clickhouse(clickhouse_client, chunk_size):\n",
    "    select_query = f'select * from test limit {chunk_size}'\n",
    "    start = time.time()\n",
    "    clickhouse_client.execute(select_query) \n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def write_vertica(conn_info, records_to_insert):   \n",
    "    start = time.time()\n",
    "    with vertica_python.connect(**conn_info) as conn:\n",
    "        cur = conn.cursor()    \n",
    "        cur.executemany(\"INSERT INTO test(id, user_id, timestamp, payload) VALUES (?, ?, ?, ?)\", records_to_insert, use_prepared_statements=True)\n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def read_vertica(conn_info, chunk_size):\n",
    "    select_query = f'select * from test limit {chunk_size}'\n",
    "    start = time.time()\n",
    "    with vertica_python.connect(**conn_info) as conn:\n",
    "        cur = conn.cursor()    \n",
    "        cur.execute(select_query) \n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b4cb5faf374426bda9078e05a89e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bf98a2a23d45f9bd680a8c39ba61b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de822bff09c4978a2b646ff6573599d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mdatabase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m db_name\n\u001b[1;32m     10\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mchunk_size\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m chunk_size\n\u001b[0;32m---> 11\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mwrite_duration_seconds\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m write(client, data_to_write)\n\u001b[1;32m     12\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mread_duration_seconds\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m read(client, chunk_size)\n\u001b[1;32m     13\u001b[0m L_results\u001b[39m.\u001b[39mappend(results)\n",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m, in \u001b[0;36mwrite_vertica\u001b[0;34m(conn_info, records_to_insert)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m vertica_python\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconn_info) \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m     21\u001b[0m     cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()    \n\u001b[0;32m---> 22\u001b[0m     cur\u001b[39m.\u001b[39;49mexecutemany(\u001b[39m\"\u001b[39;49m\u001b[39mINSERT INTO test(id, user_id, timestamp, payload) VALUES (?, ?, ?, ?)\u001b[39;49m\u001b[39m\"\u001b[39;49m, records_to_insert, use_prepared_statements\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     23\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()         \n\u001b[1;32m     24\u001b[0m duration_seconds \u001b[39m=\u001b[39m end\u001b[39m-\u001b[39mstart\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertica_python/vertica/cursor.py:183\u001b[0m, in \u001b[0;36mCursor.handle_ctrl_c.<locals>.wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    184\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertica_python/vertica/cursor.py:293\u001b[0m, in \u001b[0;36mCursor.executemany\u001b[0;34m(self, operation, seq_of_parameters, use_prepared_statements)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepared_sql \u001b[39m=\u001b[39m operation  \u001b[39m# the prepared statement is kept\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[39m# Bind the parameters and execute\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_prepared_statement(seq_of_parameters)\n\u001b[1;32m    294\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[39m#################################################################\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[39m# Rewrite the INSERT SQL into a COPY statement\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m#################################################################\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insert_statement\u001b[39m.\u001b[39mmatch(operation)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertica_python/vertica/cursor.py:987\u001b[0m, in \u001b[0;36mCursor._execute_prepared_statement\u001b[0;34m(self, list_of_parameter_values)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection\u001b[39m.\u001b[39mwrite(messages\u001b[39m.\u001b[39mFlush())\n\u001b[1;32m    986\u001b[0m \u001b[39m# Read expected message: BindComplete\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49mread_expected_message(messages\u001b[39m.\u001b[39;49mBindComplete)\n\u001b[1;32m    989\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection\u001b[39m.\u001b[39mread_message()\n\u001b[1;32m    990\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message, messages\u001b[39m.\u001b[39mErrorResponse):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertica_python/vertica/connection.py:717\u001b[0m, in \u001b[0;36mConnection.read_expected_message\u001b[0;34m(self, expected_types, error_handler)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_expected_message\u001b[39m(\u001b[39mself\u001b[39m, expected_types, error_handler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    715\u001b[0m     \u001b[39m# Reads a message and does some basic error handling.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# expected_types must be a class (e.g. messages.BindComplete) or a tuple of classes\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_message()\n\u001b[1;32m    718\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(message, expected_types):\n\u001b[1;32m    719\u001b[0m         \u001b[39mreturn\u001b[39;00m message\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertica_python/vertica/connection.py:673\u001b[0m, in \u001b[0;36mConnection.read_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m         type_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_bytes(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    674\u001b[0m         size \u001b[39m=\u001b[39m unpack(\u001b[39m'\u001b[39m\u001b[39m!I\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_bytes(\u001b[39m4\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    675\u001b[0m         \u001b[39mif\u001b[39;00m size \u001b[39m<\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertica_python/vertica/connection.py:736\u001b[0m, in \u001b[0;36mConnection.read_bytes\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_bytes\u001b[39m(\u001b[39mself\u001b[39m, n):\n\u001b[1;32m    735\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 736\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket_as_file()\u001b[39m.\u001b[39;49mread(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    737\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[1;32m    738\u001b[0m             \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mConnectionError(\u001b[39m\"\u001b[39m\u001b[39mConnection closed by Vertica\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator(SEED, N_USERS, TOTAL_RECORDS, ACTIONS) \n",
    "chunk_sizes = [1_000, 10_000, 100_000]\n",
    "L_results =[]\n",
    "for db_name, client, (write, read) in tqdm_notebook(zip(['vertica', 'clickhouse',], [conn_info,clickhouse_client], [(write_vertica, read_vertica), (write_clickhouse, read_clickhouse)]), total=2):\n",
    "    for chunk_size in tqdm_notebook(chunk_sizes):               \n",
    "        for data in data_generator.generate_chunk_data(chunk_size):  \n",
    "            data_to_write = [(row.id, row.user_id, row.timestamp, row.action) for row in data]\n",
    "            results = {}            \n",
    "            results['database'] = db_name\n",
    "            results['chunk_size'] = chunk_size\n",
    "            results['write_duration_seconds'] = write(client, data_to_write)\n",
    "            results['read_duration_seconds'] = read(client, chunk_size)\n",
    "            L_results.append(results)\n",
    "            del results, data\n",
    "            pass\n",
    "        clickhouse_client.execute('DROP TABLE IF EXISTS test;') \n",
    "        clickhouse_client.execute('CREATE TABLE IF NOT EXISTS test(id Int32, user_id Int32, timestamp Float32, payload String) ENGINE = Memory')\n",
    "        with vertica_python.connect(**conn_info) as conn:\n",
    "            cur = conn.cursor()    \n",
    "            cur.execute('DROP TABLE IF EXISTS test;') \n",
    "            cur.execute('CREATE TABLE IF NOT EXISTS test(id int, user_id int, timestamp float, payload varchar)')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>chunk_size</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>database</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clickhouse</th>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>0.050835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertica</th>\n",
       "      <td>5.015732</td>\n",
       "      <td>6.862327</td>\n",
       "      <td>6.567055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "chunk_size    1000      10000     100000\n",
       "database                                \n",
       "clickhouse  0.001943  0.006313  0.050835\n",
       "vertica     5.015732  6.862327  6.567055"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(L_results).groupby(['database','chunk_size'])['write_duration_seconds'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>chunk_size</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>database</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clickhouse</th>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.045079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertica</th>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.019803</td>\n",
       "      <td>0.004282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "chunk_size    1000      10000     100000\n",
       "database                                \n",
       "clickhouse  0.001936  0.004904  0.045079\n",
       "vertica     0.013536  0.019803  0.004282"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(L_results).groupby(['database','chunk_size'])['read_duration_seconds'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(chunk_size):\n",
    "    ids        = np.array([uuid.uuid4().hex for _ in range(chunk_size)])\n",
    "    user_ids   = np.random.choice(USER_IDS, size=chunk_size) \n",
    "    timestamps = np.int32(np.rint(np.random.uniform(0, 1e6, size=chunk_size)))\n",
    "    actions    = np.random.choice(ACTIONS, size=chunk_size)\n",
    "    data       = np.column_stack([ids, user_ids, timestamps, actions])\n",
    "    df         = pd.DataFrame(data=data, columns=['id', 'user_id', 'timestamp','action'])\n",
    "    del data \n",
    "    df['id']        = df['id'].astype('str')\n",
    "    df['user_id']   = df['user_id'].astype('str')\n",
    "    df['timestamp'] = df['timestamp'].astype('int')\n",
    "    df['action']    = df['action'].astype('str')\n",
    "    data = list(df.itertuples(index=False, name=None))\n",
    "    del df\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [\n",
    "    'start_watch',\n",
    "    'stop_watch',\n",
    "    'continue_watch',\n",
    "    'like',\n",
    "    'dislike',\n",
    "    'comment',\n",
    "    'add_to_favorite',\n",
    "    'delete_from_favorite',  \n",
    "    'user_login',\n",
    "    'user_logout'\n",
    "]\n",
    "\n",
    "def generate_random_data(chunk_size):     \n",
    "    timestamps = np.int32(np.rint(np.random.uniform(0, 1e6, size=chunk_size)))\n",
    "    actions    = np.random.choice(ACTIONS, size=chunk_size)\n",
    "    data       = np.column_stack([timestamps, actions])\n",
    "    df         = pd.DataFrame(data=data, columns=['timestamp','action'])\n",
    "    del data    \n",
    "    df['timestamp']= df['timestamp'].astype('int')\n",
    "    df['action']= df['action'].astype('str')\n",
    "    data = list(df.itertuples(index=False, name=None))\n",
    "    del df\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(204026, 'dislike'),\n",
       " (776999, 'add_to_favorite'),\n",
       " (662138, 'delete_from_favorite'),\n",
       " (756579, 'start_watch'),\n",
       " (392866, 'dislike'),\n",
       " (358562, 'continue_watch'),\n",
       " (529297, 'start_watch'),\n",
       " (375801, 'stop_watch'),\n",
       " (693043, 'stop_watch'),\n",
       " (881558, 'dislike')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_client = psycopg2.connect(\n",
    "    dbname='test', \n",
    "    user='app', \n",
    "    password='123qwe', \n",
    "    host='localhost'\n",
    ").cursor()\n",
    "\n",
    "postgres_client.execute('CREATE SCHEMA IF NOT EXISTS test;') \n",
    "postgres_client.execute('DROP TABLE IF EXISTS test.data;') \n",
    "postgres_client.execute('CREATE TABLE IF NOT EXISTS test.data(timestamp float,payload text NOT NULL);') \n",
    "\n",
    "clickhouse_client = Client('localhost')\n",
    "\n",
    "\n",
    "clickhouse_client.execute('DROP TABLE IF EXISTS test;') \n",
    "clickhouse_client.execute('CREATE TABLE IF NOT EXISTS test(timestamp Float32, payload String) ENGINE = Memory')\n",
    "\n",
    "mongo_client = pymongo.MongoClient(\"localhost\", 27017)\n",
    "db = mongo_client.test\n",
    "collection = db.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. функции чтения и записи данных в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_postgres(postgres_client, records_to_insert):\n",
    "    records_list_template = ','.join(['%s'] * len(records_to_insert))\n",
    "    insert_query = 'insert into test.data (timestamp, payload) values {}'.format(records_list_template)   \n",
    "    start = time.time()\n",
    "    postgres_client.execute(insert_query, records_to_insert)   \n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def read_postgres(postgres_client, chunk_size):\n",
    "    select_query = f'select * from test.data limit {chunk_size}'\n",
    "    start = time.time()\n",
    "    postgres_client.execute(select_query)\n",
    "    postgres_client.fetchall()     \n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def write_clickhouse(clickhouse_client, records_to_insert):\n",
    "    records_list_template = ','.join(['%s'] * len(records_to_insert))\n",
    "    insert_query = 'insert into test (timestamp, payload) values {}'.format(records_to_insert)   \n",
    "    start = time.time()\n",
    "    clickhouse_client.execute('INSERT INTO test (timestamp, payload) VALUES', records_to_insert)\n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def read_clickhouse(clickhouse_client, chunk_size):\n",
    "    select_query = f'select * from test limit {chunk_size}'\n",
    "    start = time.time()\n",
    "    clickhouse_client.execute(select_query) \n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def write_mongo(collection, records_to_insert): \n",
    "    rows = [{'timestamp':row[0], 'payload':row[1]} for row in records_to_insert]    \n",
    "    start = time.time()\n",
    "    collection.insert_many(rows) \n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds\n",
    "\n",
    "def read_mongo(collection, chunk_size):       \n",
    "    start = time.time()\n",
    "    collection.find().limit(chunk_size).skip(0)\n",
    "    end = time.time()         \n",
    "    duration_seconds = end-start\n",
    "    return duration_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. гиперпараметры эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# директория с данными\n",
    "PATH_TO_DATA = 'data'\n",
    "# число итераций\n",
    "ITERATIONS = np.arange(1, 101)\n",
    "# число записей\n",
    "TOTAL_RECORDS = 10_000_000\n",
    "# cразмер чанка для чтения/записи\n",
    "CHUNK_SIZES = [1_000, 10_000, 100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# число параллельных процессов\n",
    "POOL_SIZE=8\n",
    "# функция для мультипроцессинга\n",
    "def generate_random_data_mp(chunk_i):\n",
    "    data= generate_random_data(chunk_size)  \n",
    "    path_to_dir = f'{PATH_TO_DATA}/{chunk_size}'\n",
    "    try:\n",
    "        if not(os.path.exists(path_to_dir)):\n",
    "            os.mkdir(path_to_dir)    \n",
    "    except:\n",
    "        pass     \n",
    "    path_to_dir = f'{path_to_dir}/{iteration}'\n",
    "    try:\n",
    "        if not(os.path.exists(path_to_dir)):\n",
    "            os.mkdir(path_to_dir)\n",
    "    except:\n",
    "        pass     \n",
    "    fnm = f'{chunk_i+1}.json'      \n",
    "    with open(f'{path_to_dir}/{fnm}', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfaec5292de4a83925688e33733fc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37361360fa64b6cbcdf3570dcfaba70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk_size in tqdm_notebook(CHUNK_SIZES):\n",
    "    for iteration in tqdm_notebook(ITERATIONS):  \n",
    "        n_chunks = np.int32(np.ceil(TOTAL_RECORDS/chunk_size))  \n",
    "        chunks = np.arange(n_chunks)\n",
    "        with Pool(POOL_SIZE) as pool:\n",
    "            pool.map(generate_random_data_mp, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CLIENTS = {}\n",
    "DB_CLIENTS['postgres'] = (postgres_client, write_postgres, read_postgres)\n",
    "DB_CLIENTS['clickhouse'] = (clickhouse_client, write_clickhouse, read_clickhouse)\n",
    "DB_CLIENTS['mongo'] = (collection, write_mongo, read_mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_size in tqdm_notebook(os.listdir('data')):    \n",
    "    chunk_size = int(chunk_size)   \n",
    "    ls = os.listdir(f'data/{chunk_size}')\n",
    "    df_results = pd.DataFrame()\n",
    "    for iteration, chunk in tqdm_notebook(enumerate(ls), total = len(ls)): \n",
    "        rows=[]\n",
    "        for fnm in tqdm_notebook(os.listdir(f'data/{chunk_size}/{chunk}')):\n",
    "            pth = f'data/{chunk_size}/{chunk}/{fnm}'\n",
    "            with open(pth, 'r') as f:\n",
    "                data = json.load(f)    \n",
    "            data = list(map(lambda x: tuple(x), data))        \n",
    "            for client_name, (client, write_func, read_func) in DB_CLIENTS.items(): \n",
    "                row = {}\n",
    "                row['client'] = client_name\n",
    "                row['iteration'] = iteration+1\n",
    "                row['chunk_size'] = chunk_size\n",
    "                row['write_duration_seconds'] = write_func(client, data)\n",
    "                row['read_duration_seconds'] = read_func(client, chunk_size)\n",
    "                rows.append(row)\n",
    "                del row\n",
    "            del data\n",
    "        subdf_results = pd.DataFrame.from_records(rows)\n",
    "        del rows  \n",
    "        df_results = pd.concat([df_results, subdf_results], axis=0)  \n",
    "        del subdf_results\n",
    "        postgres_client.execute('DROP TABLE IF EXISTS test.data;') \n",
    "        postgres_client.execute('CREATE TABLE IF NOT EXISTS test.data(timestamp float,payload text NOT NULL);') \n",
    "        clickhouse_client.execute('DROP TABLE IF EXISTS test;') \n",
    "        clickhouse_client.execute('CREATE TABLE IF NOT EXISTS test(timestamp Float32, payload String) ENGINE = Memory')\n",
    "        db.test.drop()\n",
    "        collection = db.test\n",
    "    df_results.to_csv(f'df_results_{chunk_size}.csv')\n",
    "    del df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df_results_1000.csv').iloc[:, 1:]\n",
    "# df2 = pd.read_csv('df_results_10000.csv').iloc[:, 1:]\n",
    "df3 = pd.read_csv('df_results_100000.csv').iloc[:, 1:]\n",
    "df = pd.concat([df1, df3], axis= 0)\n",
    "by_keys = ['client', 'chunk_size']\n",
    "agg_keys = ['read_duration_seconds', 'write_duration_seconds']\n",
    "df_report = df.groupby(by_keys)[agg_keys].median().unstack(1)\n",
    "df_report_read = df_report['read_duration_seconds']\n",
    "df_report_read['chunk_10000/chunk_1000'] = (df_report_read.iloc[:, 1] / df_report_read.iloc[:, 0])\n",
    "df_report_write = df_report['write_duration_seconds']\n",
    "df_report_write['chunk_10000/chunk_1000'] = (df_report_write.iloc[:, 1] / df_report_write.iloc[:, 0])\n",
    "df_report_read_to_write = df_report_read.iloc[:, :-1] / df_report_write.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>chunk_size</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>chunk_10000/chunk_1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clickhouse</th>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>18.128880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mongo</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>2.146552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postgres</th>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.054052</td>\n",
       "      <td>65.922797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "chunk_size      1000    100000  chunk_10000/chunk_1000\n",
       "client                                                \n",
       "clickhouse  0.001797  0.032581               18.128880\n",
       "mongo       0.000028  0.000059                2.146552\n",
       "postgres    0.000820  0.054052               65.922797"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>chunk_size</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>chunk_10000/chunk_1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clickhouse</th>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>7.978822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mongo</th>\n",
       "      <td>0.007862</td>\n",
       "      <td>0.903528</td>\n",
       "      <td>114.929080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postgres</th>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.506959</td>\n",
       "      <td>99.070004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "chunk_size      1000    100000  chunk_10000/chunk_1000\n",
       "client                                                \n",
       "clickhouse  0.004154  0.033146                7.978822\n",
       "mongo       0.007862  0.903528              114.929080\n",
       "postgres    0.005117  0.506959               99.070004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>chunk_size</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clickhouse</th>\n",
       "      <td>0.432622</td>\n",
       "      <td>0.982970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mongo</th>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postgres</th>\n",
       "      <td>0.160229</td>\n",
       "      <td>0.106619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "chunk_size      1000    100000\n",
       "client                        \n",
       "clickhouse  0.432622  0.982970\n",
       "mongo       0.003518  0.000066\n",
       "postgres    0.160229  0.106619"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report_read_to_write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. выводы\n",
    "\n",
    "1. чтение на всех размерах чанка быстрее всего у монго\n",
    "2. при малом размере чанка чтение постгреса быстрее чтения кликхауса примерно в 10 раз, при большом размере чанка чтение кликхауса примерно в 2 раза быстрее, чем чтение постгреса\n",
    "3. запись на всех размерах чанка быстрее всего у кликхауса, причем с увеличением размера чанка кликхаус деградирует меньше всего\n",
    "4. с увеличением размера чанка чтение меньше всего деградирует у монго(x2), больше всего - у постгреса(х65)\n",
    "5. с увеличением размера чанка запись меньше всего деградирует у кликхауса(x8), больше всего - у монго(х114)\n",
    "\n",
    "### в качестве хранилища аналитических данных предпочтительнее ```clickhouse```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
